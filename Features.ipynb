{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMPpKiDQwj/EH8z3mPkLX+p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manthan-mehta/ImageClassification/blob/master/Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ernDW0k5ZgmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te0SC3yOkwhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rgb2gray(rgb):\n",
        "  return np.dot(rgb[...,:3],[0.299,0.587,0.144])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twfz_rPnlIjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.ndimage import uniform_filter\n",
        "def hog_feature(im): #Histogram of Gradient(HOG)\n",
        "  if im.ndim == 3:\n",
        "    image = rgb2gray(im)\n",
        "  else:\n",
        "    image = np.atleast_2d(im)\n",
        "\n",
        "  sx,sy = image.shape\n",
        "  orientations = 9\n",
        "  cx,cy = (8,8)\n",
        "\n",
        "  gx = np.zeros(image.shape)\n",
        "  gy = np.zeros(image.shape)\n",
        "  gx[:,:-1] = np.diff(image,n=1,axis=1) #gradient in x-direction\n",
        "  gy[:-1,:] = np.diff(image,n=1,axis=0) #gradient in y-direction\n",
        "\n",
        "  grad_mag = np.sqrt(gx ** 2 + gy ** 2) # gradient magnitude\n",
        "  grad_ori = np.arctan2(gy, (gx + 1e-15)) * (180 / np.pi) + 90 # gradient orientation\n",
        "\n",
        "  n_cellsx = int(np.floor(sx / cx))  # number of cells in x\n",
        "  n_cellsy = int(np.floor(sy / cy))  # number of cells in y\n",
        "\n",
        "  #Final feature vector of HOG\n",
        "  orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))\n",
        "  for i in range(orientations):\n",
        "    # create new integral image for this orientation\n",
        "    # isolate orientations in this range\n",
        "    temp_ori = np.where(grad_ori < 180 / orientations * (i + 1),\n",
        "                        grad_ori, 0)\n",
        "    temp_ori = np.where(grad_ori >= 180 / orientations * i,\n",
        "                        temp_ori, 0)\n",
        "    \n",
        "    # select magnitudes for those orientations\n",
        "    cond2 = temp_ori > 0\n",
        "    temp_mag = np.where(cond2, grad_mag, 0)\n",
        "    orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy].T\n",
        "  \n",
        "  return orientation_histogram.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMm-eLOAtMvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def color_histogram_hsv(im, nbin=10, xmin=0, xmax=255, normalized=True):\n",
        "  # Compute color histogram for an image using hue.\n",
        "  ndim = im.ndim\n",
        "  bins = np.linspace(xmin, xmax, nbin+1)\n",
        "  hsv = matplotlib.colors.rgb_to_hsv(im/xmax) * xmax\n",
        "  imhist, bin_edges = np.histogram(hsv[:,:,0], bins=bins, density=normalized)\n",
        "  imhist = imhist * np.diff(bin_edges)\n",
        "\n",
        "  # return histogram\n",
        "  return imhist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kng6L_HDt_si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(imgs, feature_fns, verbose=False):\n",
        "  num_images = imgs.shape[0]\n",
        "  if num_images == 0:\n",
        "    return np.array([])\n",
        "\n",
        "  # Use the first image to determine feature dimensions\n",
        "  feature_dims = []\n",
        "  first_image_features = []\n",
        "  for feature_fn in feature_fns:\n",
        "    feats = feature_fn(imgs[0].squeeze())\n",
        "    assert len(feats.shape) == 1, 'Feature functions must be one-dimensional'\n",
        "    feature_dims.append(feats.size)\n",
        "    first_image_features.append(feats)\n",
        "\n",
        "  # Now that we know the dimensions of the features, we can allocate a single\n",
        "  # big array to store all features as columns.\n",
        "  total_feature_dim = sum(feature_dims)\n",
        "  imgs_features = np.zeros((num_images, total_feature_dim))\n",
        "  imgs_features[0] = np.hstack(first_image_features).T\n",
        "\n",
        "  # Extract features for the rest of the images.\n",
        "  for i in range(1, num_images):\n",
        "    idx = 0\n",
        "    for feature_fn, feature_dim in zip(feature_fns, feature_dims):\n",
        "      next_idx = idx + feature_dim\n",
        "      imgs_features[i, idx:next_idx] = feature_fn(imgs[i].squeeze())\n",
        "      idx = next_idx\n",
        "    if verbose and i % 1000 == 0:\n",
        "      print('Done extracting features for %d / %d images' % (i, num_images))\n",
        "\n",
        "  return imgs_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Big0OJOCue-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_cifar10(cifar10_folder_path,batch_id):\n",
        "  import pickle\n",
        "  with open(cifar10_folder_path + '/data_batch_' +str(batch_id),'rb') as fo:\n",
        "    dict = pickle.load(fo,encoding='latin1')\n",
        "  return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osXsY3reuqJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = None\n",
        "Y_train = []\n",
        "for i in range(1,6):\n",
        "  dict1 = load_cifar10('/content',1)\n",
        "  if i==1:\n",
        "    train = dict1['data']\n",
        "  else:\n",
        "    train = np.vstack((train,dict1['data']))\n",
        "  Y_train += dict1['labels']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBCKfyp9u2kd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train.reshape((len(train), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "Y_train = np.asarray(Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6YE6An_u6iQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('/content/test_batch','rb') as fo:\n",
        "    test_data = pickle.load(fo,encoding='latin1')\n",
        "X_test = test_data['data'].reshape((len(test_data['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "Y_test = np.asarray(test_data['labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6KbrBqlvC6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_training = 49000\n",
        "num_validation = 1000\n",
        "num_test = 1000\n",
        "\n",
        "mask = list(range(num_training, num_training + num_validation))\n",
        "X_val = X_train[mask]\n",
        "Y_val = Y_train[mask]\n",
        "mask = list(range(num_training))\n",
        "X_train = X_train[mask]\n",
        "Y_train = Y_train[mask]\n",
        "mask = list(range(num_test))\n",
        "X_test = X_test[mask]\n",
        "Y_test = Y_test[mask]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZIm5UyBwZF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 820
        },
        "outputId": "796005d6-cc2b-4b4c-cc64-2c15c6effb75"
      },
      "source": [
        "num_color_bins = 10 # Number of bins in the color histogram\n",
        "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
        "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
        "X_val_feats = extract_features(X_val, feature_fns)\n",
        "X_test_feats = extract_features(X_test, feature_fns)\n",
        "\n",
        "# Preprocessing: Subtract the mean feature\n",
        "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
        "X_train_feats -= mean_feat\n",
        "X_val_feats -= mean_feat\n",
        "X_test_feats -= mean_feat\n",
        "\n",
        "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
        "# has roughly the same scale.\n",
        "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
        "X_train_feats /= std_feat\n",
        "X_val_feats /= std_feat\n",
        "X_test_feats /= std_feat\n",
        "\n",
        "# Preprocessing: Add a bias dimension\n",
        "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
        "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
        "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done extracting features for 1000 / 49000 images\n",
            "Done extracting features for 2000 / 49000 images\n",
            "Done extracting features for 3000 / 49000 images\n",
            "Done extracting features for 4000 / 49000 images\n",
            "Done extracting features for 5000 / 49000 images\n",
            "Done extracting features for 6000 / 49000 images\n",
            "Done extracting features for 7000 / 49000 images\n",
            "Done extracting features for 8000 / 49000 images\n",
            "Done extracting features for 9000 / 49000 images\n",
            "Done extracting features for 10000 / 49000 images\n",
            "Done extracting features for 11000 / 49000 images\n",
            "Done extracting features for 12000 / 49000 images\n",
            "Done extracting features for 13000 / 49000 images\n",
            "Done extracting features for 14000 / 49000 images\n",
            "Done extracting features for 15000 / 49000 images\n",
            "Done extracting features for 16000 / 49000 images\n",
            "Done extracting features for 17000 / 49000 images\n",
            "Done extracting features for 18000 / 49000 images\n",
            "Done extracting features for 19000 / 49000 images\n",
            "Done extracting features for 20000 / 49000 images\n",
            "Done extracting features for 21000 / 49000 images\n",
            "Done extracting features for 22000 / 49000 images\n",
            "Done extracting features for 23000 / 49000 images\n",
            "Done extracting features for 24000 / 49000 images\n",
            "Done extracting features for 25000 / 49000 images\n",
            "Done extracting features for 26000 / 49000 images\n",
            "Done extracting features for 27000 / 49000 images\n",
            "Done extracting features for 28000 / 49000 images\n",
            "Done extracting features for 29000 / 49000 images\n",
            "Done extracting features for 30000 / 49000 images\n",
            "Done extracting features for 31000 / 49000 images\n",
            "Done extracting features for 32000 / 49000 images\n",
            "Done extracting features for 33000 / 49000 images\n",
            "Done extracting features for 34000 / 49000 images\n",
            "Done extracting features for 35000 / 49000 images\n",
            "Done extracting features for 36000 / 49000 images\n",
            "Done extracting features for 37000 / 49000 images\n",
            "Done extracting features for 38000 / 49000 images\n",
            "Done extracting features for 39000 / 49000 images\n",
            "Done extracting features for 40000 / 49000 images\n",
            "Done extracting features for 41000 / 49000 images\n",
            "Done extracting features for 42000 / 49000 images\n",
            "Done extracting features for 43000 / 49000 images\n",
            "Done extracting features for 44000 / 49000 images\n",
            "Done extracting features for 45000 / 49000 images\n",
            "Done extracting features for 46000 / 49000 images\n",
            "Done extracting features for 47000 / 49000 images\n",
            "Done extracting features for 48000 / 49000 images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVqFO3aCxwnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_loss(w,x,y,reg):\n",
        "  dw = np.zeros_like(w)\n",
        "  num_train = x.shape[0]\n",
        "  loss = 0.0\n",
        "  scores = x.dot(w)\n",
        "  scores = scores - np.max(scores,axis=1,keepdims=True)\n",
        "  sum_exp_scores = np.exp(scores).sum(axis=1,keepdims=True)\n",
        "  softmax_matrix = np.exp(scores) / sum_exp_scores\n",
        "  loss = np.sum(-np.log(softmax_matrix[np.arange(num_train),y]))\n",
        "  softmax_matrix[np.arange(num_train),y] -= 1\n",
        "  dw = (x.T).dot(softmax_matrix)\n",
        "\n",
        "  loss /= num_train\n",
        "  dw /= num_train\n",
        "\n",
        "  # Regularization\n",
        "  loss += reg * np.sum(w * w)\n",
        "  dw += reg * 2 * w \n",
        "  return loss,dw \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B5l8MO3wdAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check our feature vectors with our old classifiers\n",
        "class Linear_Softmax(object):\n",
        "  def __init__(self):\n",
        "    self.W = None\n",
        "\n",
        "  def train(self, X_train, Y_train, learning_rate, reg, num_iter, batch_size = 200, verbose = False):\n",
        "    num_train,dim = X_train.shape\n",
        "    num_class = np.max(Y_train) + 1\n",
        "    if self.W is None:\n",
        "      self.W = 0.001 * np.random.randn(dim,num_class)\n",
        "    loss_history = []\n",
        "\n",
        "    #stochastic gradient descent (batch wise)\n",
        "\n",
        "    for it in range(num_iter):\n",
        "      X_batch = None\n",
        "      Y_batch = None\n",
        "      \n",
        "      batch_indices = np.random.choice(num_train, batch_size, replace=False)\n",
        "      X_batch = X_train[batch_indices]\n",
        "      Y_batch = Y_train[batch_indices]\n",
        "\n",
        "      loss,grad = softmax_loss(self.W,X_batch,Y_batch,reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      self.W = self.W - learning_rate * grad\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteration %d / %d: loss %f' % (it, num_iter, loss))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def predict(self,X):\n",
        "    scores = X.dot(self.W)\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    y_pred = np.argmax(scores,axis=1)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr2yE6LGxnNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "c1adc070-2996-42dc-fb93-27d4cea1d36d"
      },
      "source": [
        "learning_rates = [1e-3, 1e-2]\n",
        "regularization_strengths = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "np.random.seed(0)\n",
        "grid_search = [ (lr,reg) for lr in learning_rates for reg in regularization_strengths ]\n",
        "\n",
        "for lr, reg in grid_search:\n",
        "    # Create Softmax model\n",
        "    softmax_model =Linear_Softmax()\n",
        "    \n",
        "    # Train phase\n",
        "    softmax_model.train(X_train_feats, Y_train, learning_rate=lr, reg=reg, num_iter=2000,\n",
        "            batch_size=200, verbose=False)\n",
        "    y_train_pred = softmax_model.predict(X_train_feats)\n",
        "    # Train accuracy\n",
        "    train_accuracy = np.mean( y_train_pred == Y_train )\n",
        "    \n",
        "    # Validation phase\n",
        "    y_val_pred = softmax_model.predict(X_val_feats)\n",
        "    # Validation accuracy\n",
        "    val_accuracy = np.mean( y_val_pred == Y_val )\n",
        "    \n",
        "    results[lr,reg] = (train_accuracy,val_accuracy)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_accuracy > best_val:\n",
        "        best_val = val_accuracy\n",
        "        best_softmax = softmax_model\n",
        "\n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr 1.000000e-03 reg 1.000000e-02 train accuracy: 0.462388 val accuracy: 0.463000\n",
            "lr 1.000000e-03 reg 5.000000e-02 train accuracy: 0.459959 val accuracy: 0.462000\n",
            "lr 1.000000e-03 reg 1.000000e-01 train accuracy: 0.459918 val accuracy: 0.464000\n",
            "lr 1.000000e-03 reg 2.000000e-01 train accuracy: 0.456388 val accuracy: 0.462000\n",
            "lr 1.000000e-03 reg 3.000000e-01 train accuracy: 0.453735 val accuracy: 0.462000\n",
            "lr 1.000000e-03 reg 4.000000e-01 train accuracy: 0.453327 val accuracy: 0.462000\n",
            "lr 1.000000e-03 reg 5.000000e-01 train accuracy: 0.452327 val accuracy: 0.451000\n",
            "lr 1.000000e-03 reg 6.000000e-01 train accuracy: 0.448857 val accuracy: 0.456000\n",
            "lr 1.000000e-03 reg 7.000000e-01 train accuracy: 0.444837 val accuracy: 0.443000\n",
            "lr 1.000000e-02 reg 1.000000e-02 train accuracy: 0.527878 val accuracy: 0.519000\n",
            "lr 1.000000e-02 reg 5.000000e-02 train accuracy: 0.514510 val accuracy: 0.509000\n",
            "lr 1.000000e-02 reg 1.000000e-01 train accuracy: 0.500857 val accuracy: 0.508000\n",
            "lr 1.000000e-02 reg 2.000000e-01 train accuracy: 0.480347 val accuracy: 0.483000\n",
            "lr 1.000000e-02 reg 3.000000e-01 train accuracy: 0.468776 val accuracy: 0.470000\n",
            "lr 1.000000e-02 reg 4.000000e-01 train accuracy: 0.459980 val accuracy: 0.461000\n",
            "lr 1.000000e-02 reg 5.000000e-01 train accuracy: 0.454286 val accuracy: 0.450000\n",
            "lr 1.000000e-02 reg 6.000000e-01 train accuracy: 0.449102 val accuracy: 0.449000\n",
            "lr 1.000000e-02 reg 7.000000e-01 train accuracy: 0.446102 val accuracy: 0.446000\n",
            "best validation accuracy achieved during cross-validation: 0.519000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDZNH_f6ypci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}